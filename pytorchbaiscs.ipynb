{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8b9d7f",
   "metadata": {},
   "source": [
    "## importing torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac92f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b37539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a877376",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d746cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "t=torch.tensor(dd)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0df004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a810d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=t.reshape(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "786e70dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d1bc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eae6a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "t=torch.Tensor()\n",
    "print(t.dtype)\n",
    "print(t.device)\n",
    "print(t.layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81745992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ee90f",
   "metadata": {},
   "source": [
    "## constructor & factory function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9885c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) tensor([1, 2, 3], dtype=torch.int32) tensor([1, 2, 3], dtype=torch.int32) tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data=np.array([1,2,3])\n",
    "o1 = torch.Tensor(data)\n",
    "o2 = torch.tensor(data)\n",
    "o3 = torch.as_tensor(data)\n",
    "o4 = torch.from_numpy(data)\n",
    "print(o1,o2,o3,o4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676f504",
   "metadata": {},
   "source": [
    "## reshapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51a8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "t=torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n",
    "# number of elements\n",
    "print(torch.tensor(t.shape).prod())\n",
    "print(torch.numel(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17949c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "torch.Size([1, 12])\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "t=t.reshape(1,-1)\n",
    "print(t)\n",
    "print(t.shape)\n",
    "t=t.squeeze()\n",
    "print(t)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3a370",
   "metadata": {},
   "source": [
    "## flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70d1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t=t.reshape(1,-1)\n",
    "    t=t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d720ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [2, 3, 4],\n",
      "        [7, 8, 5]])\n",
      "tensor([[1, 1, 1, 2, 3, 4, 7, 8, 5]])\n",
      "tensor([1, 1, 1, 2, 3, 4, 7, 8, 5])\n",
      "tensor([[1, 1, 1, 2, 3, 4, 7, 8, 5]])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([9])\n",
      "torch.Size([1, 9])\n",
      "tensor([1, 1, 1, 2, 3, 4, 7, 8, 5])\n",
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "f=torch.tensor([[1,1,1],[2,3,4],[7,8,5]])\n",
    "t=f.reshape(1,-1)\n",
    "t1=t.squeeze()\n",
    "t2=t1.unsqueeze(dim=0)\n",
    "print(f)\n",
    "print(t)\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t.shape)\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "f=flatten(f)\n",
    "print(f)\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0e846",
   "metadata": {},
   "source": [
    "## cancatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfcc52e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [5, 6, 7],\n",
      "        [1, 2, 3],\n",
      "        [5, 6, 7]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[1, 2, 3, 1, 2, 3],\n",
      "        [5, 6, 7, 5, 6, 7]])\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "t1=torch.tensor([[1,2,3],[5,6,7]])\n",
    "t2=torch.tensor([[1,6,2],[6,6,9]])\n",
    "t3=torch.cat((t1,t1), dim=0) # row wise\n",
    "t4=torch.cat((t1,t1), dim=1) # column wise\n",
    "print(t3)\n",
    "print(t3.shape)\n",
    "print(t4)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612bf03",
   "metadata": {},
   "source": [
    "## combining gray scale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62b381a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n",
      "torch.Size([3, 1, 4, 4])\n",
      "tensor([[[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[3, 3, 3, 3],\n",
      "          [3, 3, 3, 3],\n",
      "          [3, 3, 3, 3],\n",
      "          [3, 3, 5, 3]]]])\n"
     ]
    }
   ],
   "source": [
    "#three images\n",
    "\n",
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,5,3]\n",
    "])\n",
    "\n",
    "t=torch.stack((t1,t2,t3))\n",
    "print(t.shape)\n",
    "# 3----batch size,1--------channal, \n",
    "\n",
    "t=t.reshape(3,1,4,4)\n",
    "print(t.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4861810f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "016251ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 2, 2, 2],\n",
      "         [2, 2, 2, 2],\n",
      "         [2, 2, 2, 2],\n",
      "         [2, 2, 2, 2]]])\n"
     ]
    }
   ],
   "source": [
    "print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98d07fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "355b2da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(t[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d26b06d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# acces 5\n",
    "print(t[2][0][3][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "908b6340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=t.reshape(1,-1).squeeze()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8269fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=t.reshape(t.shape[0],(torch.numel(t)//t.shape[0])).squeeze()\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cff221e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "t.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956b211",
   "metadata": {},
   "source": [
    "## Element-Wise Tensor Operations For Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9755d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1=torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float32)\n",
    "t2=torch.tensor([[1,2,3],[4,7,6]] ,dtype=torch.float32)\n",
    "t=t1+t2\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=t1-t2\n",
    "print(t)\n",
    "t=t1*t2\n",
    "print(t)\n",
    "t=t1/t2\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34325bae",
   "metadata": {},
   "source": [
    "### broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd02b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=t1-2\n",
    "print(t)\n",
    "t=t1*2\n",
    "print(t)\n",
    "t=t1/2\n",
    "print(t)\n",
    "print(t1.add(2))\n",
    "print(t1.sub(2))\n",
    "print(t1.mul(2))\n",
    "print(t1.div(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t3=torch.tensor(np.broadcast_to(2, t1.shape),dtype=torch.float32)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=t1+t3\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,1],\n",
    "    [1,1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([2,4], dtype=torch.float32)\n",
    "t5=torch.tensor(np.broadcast_to(t2, t1.shape),dtype=torch.float32)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800754cd",
   "metadata": {},
   "source": [
    "### comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445adc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1>t2)\n",
    "print(t1.eq(0))\n",
    "print(t1<t2)\n",
    "print(t1.ge(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.abs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e821b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030ccc1",
   "metadata": {},
   "source": [
    "## Tensor Reduction Ops For Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float32)\n",
    "print(t.sum())\n",
    "print(t.sum().numel()<t.numel())\n",
    "t.prod()\n",
    "t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa94b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t[0])\n",
    "print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41411e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.max()\n",
    "print(t.max())\n",
    "print(t.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a024fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.mean())\n",
    "t.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacf972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.mean(dim=0).tolist())\n",
    "t.mean(dim=0).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca53716",
   "metadata": {},
   "source": [
    "## data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9525302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03760b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(train_set,batch_size=100,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe984b2",
   "metadata": {},
   "source": [
    "## data visulaizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58482ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "loader=torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=next(iter(train_set))\n",
    "print(type(sample))\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a270c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image,label=sample\n",
    "plt.imshow(image.squeeze(),cmap='gray')\n",
    "plt.xlabel(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disply a batch\n",
    "laoder= torch.utils.data.DataLoader(train_set, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(laoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d262c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bedcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "images,labels=batch\n",
    "images[0].shape \n",
    "grid=torchvision.utils.make_grid(images, nrow=5)\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "plt.xlabel(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module): # line 1\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__() # line 3   \n",
    "        # The super() builtin returns a proxy object (temporary object of the superclass) that allows us to access methods of the base class.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.layer(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.Con1=nn.Conv2d(in_channels= 1    ,out_channels=6, kernel_size=5)\n",
    "        self.Con2=nn.Conv2d(in_channels= 6    ,out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    def forward(self,t):\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f1a07",
   "metadata": {},
   "source": [
    "## PyTorch CNN Layer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Network()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5694b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.Con1\n",
    "network.Con1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e475b",
   "metadata": {},
   "source": [
    "## PyTorch Callable Neural Networks - Deep Learning In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "\n",
    "weight_matrix = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70af68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "weight_matrix = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "weight_matrix.matmul(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b92b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc=nn.Linear(in_features=4 , out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc(in_features)\n",
    "fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a769bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight = nn.Parameter(weight_matrix)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54adf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network1,self).__init__()\n",
    "        self.Con1=nn.Conv2d(in_channels= 1    ,out_channels=6, kernel_size=5)\n",
    "        self.Con2=nn.Conv2d(in_channels= 6    ,out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    def forward(self,t):\n",
    "        # 1 layer\n",
    "        t=t\n",
    "        # 2layer\n",
    "        t=self.Con1(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t,kernel_size=2, stride=2)\n",
    "        #3 layer\n",
    "        t=self.Con2(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t,kernel_size=2, stride=2)\n",
    "        #4layer\n",
    "        t=t.reshape(-1,12*4*4)\n",
    "        t=self.fc1(t)\n",
    "        t=F.relu(t)\n",
    "        #5 layer\n",
    "        t=self.fc2(t)\n",
    "        t=F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da52529",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=next(iter(train_set))\n",
    "image,label=sample\n",
    "image.shape \n",
    "image.unsqueeze(0).shape\n",
    "\n",
    "\n",
    "g=image.unsqueeze(0)\n",
    "g.shape\n",
    "network=Network1()\n",
    "pred = network(g) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf162a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932eb80",
   "metadata": {},
   "source": [
    "## for batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "laoder= torch.utils.data.DataLoader(train_set, batch_size=20)\n",
    "batch=next(iter(laoder))\n",
    "images,labels=batch\n",
    "images.shape \n",
    "\n",
    "\n",
    "\n",
    "network=Network1()\n",
    "pred = network(images) \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1db0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eac720",
   "metadata": {},
   "source": [
    "## back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9484a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_set=torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "laoder= torch.utils.data.DataLoader(train_set, batch_size=20)\n",
    "batch=next(iter(laoder))\n",
    "images,labels=batch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283e3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network2,self).__init__()\n",
    "        self.con1= nn.Conv2d(in_channels=1   ,out_channels=6, kernel_size=5)\n",
    "        self.con2= nn.Conv2d(in_channels=6,   out_channels=12, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60,  out_features=10)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self,t):\n",
    "        # 1 layer\n",
    "        t=t\n",
    "        # 2layer\n",
    "        t=self.con1(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t,kernel_size=2, stride=2)\n",
    "        #3 layer\n",
    "        t=self.con2(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t,kernel_size=2, stride=2)\n",
    "        #4layer\n",
    "        t=t.reshape(-1,12*4*4)\n",
    "        t=self.fc1(t)\n",
    "        t=F.relu(t)\n",
    "        #5 layer\n",
    "        t=self.fc2(t)\n",
    "        t=F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "517534fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "laoder= torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "batch=next(iter(laoder))\n",
    "images,labels=batch\n",
    "\n",
    "network=Network2()\n",
    "preds = network(images) \n",
    "\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item\n",
    "loss.backward()\n",
    "network.con1.weight.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e58663b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2984299659729004"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0ae5dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2562143802642822"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "optimizer.step() # Updating the weights\n",
    "preds = network(images)\n",
    "\n",
    "\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7fd9c7",
   "metadata": {},
   "source": [
    "## complete code for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab69c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 353.0764392763376\n",
      "epoch 1 loss: 235.32924690842628\n",
      "epoch 2 loss: 213.9722430408001\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "network=Network2()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    ,batch_size=100\n",
    "    ,shuffle=True\n",
    ")\n",
    "\n",
    "for ep in range(3):\n",
    "    total_loss=0\n",
    "   \n",
    "    for batch in train_loader:\n",
    "        images,labels=batch\n",
    "        preds=network(images) \n",
    "        loss = F.cross_entropy(preds, labels) #calculte loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(\"epoch\", ep, \"loss:\", total_loss)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18556de",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "834cedef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(train_set.targets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "868017db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model,loader,):\n",
    "    all_pred=torch.tensor([])\n",
    "    for batch in laoder:\n",
    "        images,labels=batch\n",
    "        preds=network(images)\n",
    "        all_pred=torch.cat((all_pred,preds),dim=0)\n",
    "    return all_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "846f3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
    "    train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7cb8c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb05066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked=torch.stack((train_set.targets,train_preds.argmax(dim=1)),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6539fcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 9],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [3, 3],\n",
       "        [0, 0],\n",
       "        [5, 5]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92d7299d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmt= torch.zeros((10,10), dtype=torch.float32)\n",
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e60efebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in stacked:\n",
    "    j,k=p.tolist()\n",
    "    cmt[j,k]=cmt[j,k]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56f9b597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.3260e+03, 5.0000e+00, 8.1000e+01, 2.0900e+02, 1.6000e+01, 4.0000e+00,\n",
       "         2.7400e+02, 2.0000e+00, 8.2000e+01, 1.0000e+00],\n",
       "        [1.0000e+01, 5.8050e+03, 1.0000e+00, 1.4100e+02, 1.3000e+01, 1.0000e+00,\n",
       "         1.2000e+01, 0.0000e+00, 1.7000e+01, 0.0000e+00],\n",
       "        [9.6000e+01, 0.0000e+00, 4.6890e+03, 9.7000e+01, 7.6200e+02, 1.0000e+00,\n",
       "         2.8700e+02, 0.0000e+00, 6.8000e+01, 0.0000e+00],\n",
       "        [1.5300e+02, 2.4000e+01, 1.5000e+01, 5.5640e+03, 1.3700e+02, 1.0000e+00,\n",
       "         9.5000e+01, 0.0000e+00, 1.1000e+01, 0.0000e+00],\n",
       "        [1.1000e+01, 5.0000e+00, 3.8700e+02, 3.2200e+02, 4.9590e+03, 1.0000e+00,\n",
       "         2.8300e+02, 1.0000e+00, 3.1000e+01, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 5.7860e+03,\n",
       "         0.0000e+00, 1.5000e+02, 6.0000e+00, 5.7000e+01],\n",
       "        [1.2520e+03, 9.0000e+00, 5.6600e+02, 1.8300e+02, 6.3400e+02, 1.0000e+00,\n",
       "         3.1820e+03, 0.0000e+00, 1.7300e+02, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4000e+01,\n",
       "         0.0000e+00, 5.8560e+03, 1.0000e+00, 8.9000e+01],\n",
       "        [9.0000e+00, 1.0000e+00, 1.6000e+01, 2.5000e+01, 1.6000e+01, 3.1000e+01,\n",
       "         2.4000e+01, 1.6000e+01, 5.8590e+03, 3.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 2.9000e+01,\n",
       "         0.0000e+00, 3.0700e+02, 7.0000e+00, 5.6560e+03]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f941551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting resource\n",
      "  Downloading Resource-0.2.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting JsonSir>=0.0.2\n",
      "  Downloading JsonSir-0.0.2.tar.gz (2.2 kB)\n",
      "Collecting JsonForm>=0.0.2\n",
      "  Downloading JsonForm-0.0.2.tar.gz (2.4 kB)\n",
      "Collecting python-easyconfig>=0.1.0\n",
      "  Downloading Python_EasyConfig-0.1.7-py2.py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from JsonForm>=0.0.2->resource) (4.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from python-easyconfig>=0.1.0->resource) (6.0)\n",
      "Requirement already satisfied: six in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from python-easyconfig>=0.1.0->resource) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from jsonschema->JsonForm>=0.0.2->resource) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from jsonschema->JsonForm>=0.0.2->resource) (0.18.0)\n",
      "Building wheels for collected packages: JsonForm, JsonSir\n",
      "  Building wheel for JsonForm (setup.py): started\n",
      "  Building wheel for JsonForm (setup.py): finished with status 'done'\n",
      "  Created wheel for JsonForm: filename=JsonForm-0.0.2-py3-none-any.whl size=3325 sha256=f8c9fd4385ad09cc0d1cb6bdb69f0197b9c3745af9b5cc8cd2998ad9f88240eb\n",
      "  Stored in directory: c:\\users\\pssrl\\appdata\\local\\pip\\cache\\wheels\\2e\\bd\\ab\\c1026535edf314ce2b0d7ba3b2dd0ca67bfb5bae2cb301510f\n",
      "  Building wheel for JsonSir (setup.py): started\n",
      "  Building wheel for JsonSir (setup.py): finished with status 'done'\n",
      "  Created wheel for JsonSir: filename=JsonSir-0.0.2-py3-none-any.whl size=4769 sha256=9a6394731d6a4d113f8c6d49eb9bfd26ed764dd0059cd0b06406bac9137c4299\n",
      "  Stored in directory: c:\\users\\pssrl\\appdata\\local\\pip\\cache\\wheels\\bb\\e7\\72\\08831f4f1927bfcb155f0a971e253bfe677172cebea1332b51\n",
      "Successfully built JsonForm JsonSir\n",
      "Installing collected packages: python-easyconfig, JsonSir, JsonForm, resource\n",
      "Successfully installed JsonForm-0.0.2 JsonSir-0.0.2 python-easyconfig-0.1.7 resource-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67aee0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resources'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresources\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotcm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'resources'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from resources.plotcm import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "878a1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19c45ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "tensor([[5.3260e+03, 5.0000e+00, 8.1000e+01, 2.0900e+02, 1.6000e+01, 4.0000e+00,\n",
      "         2.7400e+02, 2.0000e+00, 8.2000e+01, 1.0000e+00],\n",
      "        [1.0000e+01, 5.8050e+03, 1.0000e+00, 1.4100e+02, 1.3000e+01, 1.0000e+00,\n",
      "         1.2000e+01, 0.0000e+00, 1.7000e+01, 0.0000e+00],\n",
      "        [9.6000e+01, 0.0000e+00, 4.6890e+03, 9.7000e+01, 7.6200e+02, 1.0000e+00,\n",
      "         2.8700e+02, 0.0000e+00, 6.8000e+01, 0.0000e+00],\n",
      "        [1.5300e+02, 2.4000e+01, 1.5000e+01, 5.5640e+03, 1.3700e+02, 1.0000e+00,\n",
      "         9.5000e+01, 0.0000e+00, 1.1000e+01, 0.0000e+00],\n",
      "        [1.1000e+01, 5.0000e+00, 3.8700e+02, 3.2200e+02, 4.9590e+03, 1.0000e+00,\n",
      "         2.8300e+02, 1.0000e+00, 3.1000e+01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 5.7860e+03,\n",
      "         0.0000e+00, 1.5000e+02, 6.0000e+00, 5.7000e+01],\n",
      "        [1.2520e+03, 9.0000e+00, 5.6600e+02, 1.8300e+02, 6.3400e+02, 1.0000e+00,\n",
      "         3.1820e+03, 0.0000e+00, 1.7300e+02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4000e+01,\n",
      "         0.0000e+00, 5.8560e+03, 1.0000e+00, 8.9000e+01],\n",
      "        [9.0000e+00, 1.0000e+00, 1.6000e+01, 2.5000e+01, 1.6000e+01, 3.1000e+01,\n",
      "         2.4000e+01, 1.6000e+01, 5.8590e+03, 3.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 2.9000e+01,\n",
      "         0.0000e+00, 3.0700e+02, 7.0000e+00, 5.6560e+03]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'd' for object of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mplot_confusion_matrix\u001b[1;34m(cm, classes, normalize, title, cmap)\u001b[0m\n\u001b[0;32m     21\u001b[0m thresh \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;28mrange\u001b[39m(cm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mrange\u001b[39m(cm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])):\n\u001b[1;32m---> 23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtext(j, i, \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m, horizontalalignment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cm[i, j] \u001b[38;5;241m>\u001b[39m thresh \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:659\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta:\n\u001b[1;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown format code 'd' for object of type 'float'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cmt, train_set.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c11f151",
   "metadata": {},
   "source": [
    "## TensorBoard: TensorFlow's Visualization Toolkit\n",
    "TensorBoard provides the visualization and tooling needed for machine learning experimentation:\n",
    "\n",
    "Tracking and visualizing metrics such as loss and accuracy\n",
    "Visualizing the model graph (ops and layers)\n",
    "Viewing histograms of weights, biases, or other tensors as they change over time\n",
    "Projecting embeddings to a lower dimensional space\n",
    "Displaying images, text, and audio data\n",
    "Profiling TensorFlow programs\n",
    "And much more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fdc22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9d7b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (1.21.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (1.42.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (3.19.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (2.27.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (1.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pssrl\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9116a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5cd197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb=SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6e20fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Network2()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    ,batch_size=100\n",
    "    ,shuffle=True\n",
    ")\n",
    "images, labels=next(iter(train_loader))\n",
    "\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network, images)\n",
    "tb.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d463cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 47366 loss: 334.6899947375059\n",
      "epoch 1 total_correct: 51679 loss: 226.06625418365002\n",
      "epoch 2 total_correct: 52230 loss: 208.71307377517223\n",
      "epoch 3 total_correct: 52533 loss: 200.02272686362267\n",
      "epoch 4 total_correct: 52685 loss: 196.75627595186234\n"
     ]
    }
   ],
   "source": [
    "network = Network2()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network, images)\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader: # Get Batch\n",
    "        images,labels= batch\n",
    "        preds=network(images)\n",
    "        loss=F.cross_entropy(preds,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += preds.argmax(dim=1).eq(labels).sum().item()\n",
    "    \n",
    "\n",
    "    tb.add_scalar('Loss', total_loss, epoch)\n",
    "    tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "    tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "\n",
    "    tb.add_histogram('conv1.bias', network.con1.bias, epoch)\n",
    "    tb.add_histogram('conv1.weight', network.con1.weight, epoch)\n",
    "    tb.add_histogram( 'conv1.weight.grad',network.con1.weight.grad,epoch)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )\n",
    "\n",
    "tb.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8f0a8",
   "metadata": {},
   "source": [
    "## Experimenting With Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0307e02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 47457 loss: 32891.66320860386\n",
      "epoch 1 total_correct: 51583 loss: 22939.468251168728\n",
      "epoch 2 total_correct: 52241 loss: 20895.06400823593\n",
      "epoch 3 total_correct: 52666 loss: 19979.696530103683\n",
      "epoch 4 total_correct: 52826 loss: 19403.376802802086\n",
      "epoch 0 total_correct: 42276 loss: 46173.58047962189\n",
      "epoch 1 total_correct: 48418 loss: 30513.582208752632\n",
      "epoch 2 total_correct: 50497 loss: 26159.521636366844\n",
      "epoch 3 total_correct: 51414 loss: 23624.895583093166\n",
      "epoch 4 total_correct: 52083 loss: 21836.83896958828\n",
      "epoch 0 total_correct: 33657 loss: 81807.77510404587\n",
      "epoch 1 total_correct: 42857 loss: 45294.28866505623\n",
      "epoch 2 total_correct: 44293 loss: 41026.06430053711\n",
      "epoch 3 total_correct: 45198 loss: 38304.61553633213\n",
      "epoch 4 total_correct: 46026 loss: 36298.76320362091\n",
      "epoch 0 total_correct: 6000 loss: 137791.2318468094\n",
      "epoch 1 total_correct: 19398 loss: 132139.34297561646\n",
      "epoch 2 total_correct: 25250 loss: 111249.09261465073\n",
      "epoch 3 total_correct: 33134 loss: 87020.3553199768\n",
      "epoch 4 total_correct: 36525 loss: 73085.46868562698\n",
      "epoch 0 total_correct: 37337 loss: 59147.7085351944\n",
      "epoch 1 total_correct: 47239 loss: 32970.75432538986\n",
      "epoch 2 total_correct: 50041 loss: 26824.61169362068\n",
      "epoch 3 total_correct: 51261 loss: 23862.382501363754\n",
      "epoch 4 total_correct: 52185 loss: 21373.446822166443\n",
      "epoch 0 total_correct: 28941 loss: 92213.449716568\n",
      "epoch 1 total_correct: 43150 loss: 45050.44835805893\n",
      "epoch 2 total_correct: 45359 loss: 38358.788311481476\n",
      "epoch 3 total_correct: 46735 loss: 35126.644134521484\n",
      "epoch 4 total_correct: 47635 loss: 32985.27428507805\n",
      "epoch 0 total_correct: 8041 loss: 137312.11638450623\n",
      "epoch 1 total_correct: 20314 loss: 130369.60935592651\n",
      "epoch 2 total_correct: 33185 loss: 103092.09191799164\n",
      "epoch 3 total_correct: 37932 loss: 71344.85149383545\n",
      "epoch 4 total_correct: 39941 loss: 57363.24495077133\n",
      "epoch 0 total_correct: 6000 loss: 138249.8278617859\n",
      "epoch 1 total_correct: 6000 loss: 138154.2100906372\n",
      "epoch 2 total_correct: 5998 loss: 138043.09582710266\n",
      "epoch 3 total_correct: 6004 loss: 137899.35207366943\n",
      "epoch 4 total_correct: 6134 loss: 137697.54791259766\n",
      "epoch 0 total_correct: 10963 loss: 131044.7347164154\n",
      "epoch 1 total_correct: 21574 loss: 97371.9334602356\n",
      "epoch 2 total_correct: 31445 loss: 69904.1759967804\n",
      "epoch 3 total_correct: 36818 loss: 59980.06761074066\n",
      "epoch 4 total_correct: 39371 loss: 53452.39877700806\n",
      "epoch 0 total_correct: 7852 loss: 137826.12323760986\n",
      "epoch 1 total_correct: 13572 loss: 135841.31479263306\n",
      "epoch 2 total_correct: 22698 loss: 130515.99979400635\n",
      "epoch 3 total_correct: 21539 loss: 118477.27417945862\n",
      "epoch 4 total_correct: 26031 loss: 99064.01038169861\n",
      "epoch 0 total_correct: 6876 loss: 138272.25923538208\n",
      "epoch 1 total_correct: 6569 loss: 138188.09986114502\n",
      "epoch 2 total_correct: 6471 loss: 138096.91905975342\n",
      "epoch 3 total_correct: 6427 loss: 137991.05644226074\n",
      "epoch 4 total_correct: 6408 loss: 137862.23649978638\n",
      "epoch 0 total_correct: 6000 loss: 138299.31020736694\n",
      "epoch 1 total_correct: 6000 loss: 138285.37225723267\n",
      "epoch 2 total_correct: 6000 loss: 138271.82054519653\n",
      "epoch 3 total_correct: 6000 loss: 138258.65268707275\n",
      "epoch 4 total_correct: 6000 loss: 138245.84484100342\n"
     ]
    }
   ],
   "source": [
    "batch_size_list = [100, 1000, 10000]\n",
    "lr_list = [.01, .001, .0001, .00001]\n",
    "for batch_size in batch_size_list:\n",
    "    for lr in lr_list:\n",
    "        network = Network2()\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_set, batch_size=batch_size\n",
    "        )\n",
    "        optimizer = optim.Adam(\n",
    "            network.parameters(), lr=lr\n",
    "        )\n",
    "\n",
    "        images, labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        comment=f' batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment=comment)\n",
    "        tb.add_image('images', grid)\n",
    "        tb.add_graph(network, images)\n",
    "\n",
    "        for epoch in range(5):\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "            for batch in train_loader:\n",
    "                images, labels = batch # Get Batch\n",
    "                preds = network(images) # Pass Batch\n",
    "                loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "                optimizer.zero_grad() # Zero Gradients\n",
    "                loss.backward() # Calculate Gradients\n",
    "                optimizer.step() # Update Weights\n",
    "\n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_correct += preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "            tb.add_scalar(\n",
    "                'Loss', total_loss, epoch\n",
    "            )\n",
    "            tb.add_scalar(\n",
    "                'Number Correct', total_correct, epoch\n",
    "            )\n",
    "            tb.add_scalar(\n",
    "                'Accuracy', total_correct / len(train_set), epoch\n",
    "            )\n",
    "\n",
    "            for name, param in network.named_parameters():\n",
    "                tb.add_histogram(name, param, epoch)\n",
    "                tb.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "\n",
    "            print(\n",
    "                \"epoch\", epoch\n",
    "                ,\"total_correct:\", total_correct\n",
    "                ,\"loss:\", total_loss\n",
    "            )  \n",
    "        tb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85626c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 100 True\n",
      "0.01 100 False\n",
      "0.01 1000 True\n",
      "0.01 1000 False\n",
      "0.001 100 True\n",
      "0.001 100 False\n",
      "0.001 1000 True\n",
      "0.001 1000 False\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "parameters = dict(\n",
    "    lr = [.01, .001]\n",
    "    ,batch_size = [100, 1000]\n",
    "    ,shuffle = [True, False]\n",
    ")\n",
    "\n",
    "param_values=[v for v in parameters.values()]\n",
    "for lr, batch_size, shuffle in product(*param_values): \n",
    "    print (lr, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637ad90",
   "metadata": {},
   "source": [
    "## Using The RunBuilder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8acfa160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd41f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.01, .001]\n",
    "    ,batch_size = [1000, 10000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7da541d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = RunBuilder.get_runs(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c50a3976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(lr=0.01, batch_size=1000),\n",
       " Run(lr=0.01, batch_size=10000),\n",
       " Run(lr=0.001, batch_size=1000),\n",
       " Run(lr=0.001, batch_size=10000)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4b6e335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(lr=0.01, batch_size=1000, device='cuda'),\n",
       " Run(lr=0.01, batch_size=1000, device='cpu'),\n",
       " Run(lr=0.01, batch_size=10000, device='cuda'),\n",
       " Run(lr=0.01, batch_size=10000, device='cpu'),\n",
       " Run(lr=0.001, batch_size=1000, device='cuda'),\n",
       " Run(lr=0.001, batch_size=1000, device='cpu'),\n",
       " Run(lr=0.001, batch_size=10000, device='cuda'),\n",
       " Run(lr=0.001, batch_size=10000, device='cpu')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.01, .001]\n",
    "    ,batch_size = [1000, 10000]\n",
    "    ,device = [\"cuda\", \"cpu\"]\n",
    ")\n",
    "\n",
    "runs = RunBuilder.get_runs(params)\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96e04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

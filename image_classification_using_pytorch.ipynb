{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5d08bf",
   "metadata": {},
   "source": [
    "## include libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae825e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae25f0b",
   "metadata": {},
   "source": [
    "## download train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356068a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad276dcf",
   "metadata": {},
   "source": [
    "## visilaize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645137e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "loader=torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "image,label=next(iter(train_set))\n",
    "plt.imshow(image.squeeze(),cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(loader))\n",
    "images,labels= batch\n",
    "grid=torchvision.utils.make_grid(images, nrow=5)\n",
    "plt.imshow(np.transpose(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa41504",
   "metadata": {},
   "source": [
    "## neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.con1=nn.Conv2d(in_channels=1, out_channels=6 ,kernel_size=5)\n",
    "        self.con2=nn.Conv2d(in_channels=6, out_channels=12  ,kernel_size=5)\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2=nn.Linear(in_features=120 , out_features=60)\n",
    "        self.out=nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self,t):\n",
    "        #1st layer\n",
    "        t=t\n",
    "        #2nd layer\n",
    "        t=self.con1(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t,kernel_size=2, stride=2)\n",
    "        #3rd layer\n",
    "        \n",
    "        t=self.con2(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t,kernel_size=2, stride=2)\n",
    "        \n",
    "        #4th layer\n",
    "        t=t.reshape(-1,12*4*4)\n",
    "        t=self.fc1(t)\n",
    "        t=F.relu(t)\n",
    "        \n",
    "        #5th layer\n",
    "       \n",
    "        t=self.fc2(t)\n",
    "        t=F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05281535",
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Network()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad0c5e",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "pred=network(image)\n",
    "pred.argmax(dim=1).eq(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e87923",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=network(images)\n",
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36c4a6",
   "metadata": {},
   "source": [
    "## training, backpropagation and loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=torch.utils.data.DataLoader(train_set,batch_size=100, shuffle=True)\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer=optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for ep in range(3):\n",
    "    total_loss=0\n",
    "    for batch in loader:\n",
    "        images,labels=batch\n",
    "        preds=network(images)\n",
    "        loss=F.cross_entropy(preds,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    print(\"epoch\", ep, \"loss:\", total_loss)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181ab57",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model, laoder):\n",
    "    all_pred=torch.tensor([])\n",
    "    for batch in laoder:\n",
    "        images,labels=batch\n",
    "        preds=network(images)\n",
    "        all_pred=torch.cat((all_pred,preds),dim=0)\n",
    "    return all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372780ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "    train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7486e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt=torch.zeros([10,10])\n",
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked=torch.stack((train_set.targets,train_preds.argmax(dim=1)),dim=1)\n",
    "for p in stacked:\n",
    "    j,k=p.tolist()\n",
    "    cmt[j,k]=cmt[j,k]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50923a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt\n",
    "#plot_confusion_matrix(cmt, train_set.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90a300",
   "metadata": {},
   "source": [
    "## tensor borad for visializtion of result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb=SummaryWriter()\n",
    "network1=Network()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set\n",
    "    ,batch_size=100\n",
    "    ,shuffle=True\n",
    ")\n",
    "images, labels=next(iter(train_loader))\n",
    "grid=torchvision.utils.make_grid(images)\n",
    "tb.add_image('images',grid)\n",
    "tb.add_graph(network1,images)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d2273",
   "metadata": {},
   "source": [
    "## Access the TensorBoard UI by browsing to http://localhost:6006 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f54616",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=torch.utils.data.DataLoader(train_set,batch_size=100, shuffle=True)\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer=optim.Adam(network.parameters(), lr=0.01)\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network1, images)\n",
    "\n",
    "for ep in range(3):\n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    for batch in loader:\n",
    "        images,labels=batch\n",
    "        preds=network(images)\n",
    "        loss=F.cross_entropy(preds,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "        total_correct += preds.argmax(dim=1).eq(labels).sum().item()\n",
    "    tb.add_scalar('Loss', total_loss, ep)\n",
    "    tb.add_scalar('Number Correct', total_correct, ep)\n",
    "    tb.add_histogram('conv1.bias', network.con1.bias, ep)\n",
    "    print(\"epoch\", ep, \"loss:\", total_loss)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5884d",
   "metadata": {},
   "source": [
    "## Experimenting With Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7939e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [100, 1000, 10000]\n",
    "lr_list = [.01, .001, .0001, .00001]\n",
    "for batch_size in batch_size_list:\n",
    "    for lr in lr_list:\n",
    "        network = Network()\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_set, batch_size=batch_size\n",
    "        )\n",
    "        optimizer = optim.Adam(\n",
    "            network.parameters(), lr=lr\n",
    "        )\n",
    "\n",
    "        images, labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        comment=f' batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment=comment)\n",
    "        tb.add_image('images', grid)\n",
    "        tb.add_graph(network, images)\n",
    "\n",
    "        for epoch in range(1):\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "            for batch in train_loader:\n",
    "                images, labels = batch # Get Batch\n",
    "                preds = network(images) # Pass Batch\n",
    "                loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "                optimizer.zero_grad() # Zero Gradients\n",
    "                loss.backward() # Calculate Gradients\n",
    "                optimizer.step() # Update Weights\n",
    "\n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_correct += preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "            tb.add_scalar(\n",
    "                'Loss', total_loss, epoch\n",
    "            )\n",
    "            tb.add_scalar(\n",
    "                'Number Correct', total_correct, epoch\n",
    "            )\n",
    "            tb.add_scalar(\n",
    "                'Accuracy', total_correct / len(train_set), epoch\n",
    "            )\n",
    "\n",
    "            for name, param in network.named_parameters():\n",
    "                tb.add_histogram(name, param, epoch)\n",
    "                tb.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "\n",
    "            print(\n",
    "                \"epoch\", epoch\n",
    "                ,\"total_correct:\", total_correct\n",
    "                ,\"loss:\", total_loss\n",
    "            )  \n",
    "        tb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3216633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
